[{"authors":["admin"],"categories":null,"content":"Gyunam Park is currently a Ph.D. student at the chair of Process and Data Science (PADS), Computer Science, RWTH-Aahcen University since 2019. His research interest includes process mining, data science, online operational support, machine learning, and deep learning. He received M.Sc. degree in in the Department of Industrial \u0026amp; Management Engineering at POSTECH (Pohang University of Science and Technology), Pohang, South Korea. He received the B.S. degree in business administration and computer science from UNIST (Ulsan National Institute of Science and Technology), Ulsan, South Korea.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://gyunamister.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Gyunam Park is currently a Ph.D. student at the chair of Process and Data Science (PADS), Computer Science, RWTH-Aahcen University since 2019. His research interest includes process mining, data science, online operational support, machine learning, and deep learning. He received M.Sc. degree in in the Department of Industrial \u0026amp; Management Engineering at POSTECH (Pohang University of Science and Technology), Pohang, South Korea. He received the B.S. degree in business administration and computer science from UNIST (Ulsan National Institute of Science and Technology), Ulsan, South Korea.","tags":null,"title":"Gyunam Park","type":"authors"},{"authors":[],"categories":[],"content":"","date":1583020800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583020800,"objectID":"1c84e1d124e5cd010cafa725d72d98c3","permalink":"https://gyunamister.github.io/publication/wi-2020/","publishdate":"2020-01-23T20:12:43+01:00","relpermalink":"/publication/wi-2020/","section":"publication","summary":"Predictive business process monitoring (PBPM) deals with predicting a process's future behavior based on historical event logs to support a process's execution. Many of the recent techniques utilize a machine-learned model to predict which event type is the next most likely. Beyond PBPM, prescriptive BPM aims at finding optimal actions based on considering relevant key performance indicators. Existing techniques are geared towards the outcome prediction and deal with alarms for interventions or interventions that do not represent process events. In this paper, we argue that the next event prediction is insufficient for practitioners. Accordingly, this research-in-progress paper proposes a technique for determining next best actions that represent process events. We conducted an intermediate evaluation to test the usefulness and the quality of our technique compared to the most frequently cited technique for predicting next events. The results show a higher usefulness for process participants than a next most likely event.","tags":["Paper","Recommendation","WI2020"],"title":"From predictive to prescriptive process monitoring: Recommending the next best actions instead of calculating the next most likely events","type":"publication"},{"authors":["Gyunam Park"],"categories":[],"content":"The (easiest) Hadoop installation - MacOS/Linux (Last updated: 24. Jan. 2020 14:00)\nThis blog post is a supplement for Hadoop instruction at Introduction to Data Science, RWTH-Aachen. The goal is to make sure that everyone can run Hadoop at his/her PC. Thus, I aim for explaining how to install Hadoop in the easiest manner by taking the simplest approach. This is not a succint way to install Hadoop (If you think you are quite familar with how (system-related) things work, I recommend you to find other blog posts).\nLet's get started.\n1. Install Java Hadoop is based on Java language, so you need to have Java SE Development Kit in your PC.\nYou can download it in the link below:\nhttps://www.oracle.com/technetwork/java/javase/downloads/jdk8downloads-2133151.html\n Take note of the installation directory. E.g., /Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/bin  2. Download hadoop Download hadoop and unzip it at /usr/local/Cellar/hadoop-2.8.4\n3. Configure System Environment We need to set Hadoop PATH in system environment. This step is required to use hadoop commands at Terminal (zsh).\nOpen Terminal (zsh)\n$ vi /etc/profile (/etc/zprofile) Insert below sentences to the file. (You can change to edit mode by typing i)\nexport HADOOP_HOME=/usr/local/Cellar/hadoop-2.8.4 export PATH=$PATH:$HADOOP_HOME/bin export PATH=$PATH:$HADOOP_HOME/sbin At your Terminal (zsh)\nsource /etc/profile (/etc/zprofile) echo $HADOOP_HOME //YOU MUST SEE /usr/local/Cellar/hadoop-2.8.4 on your terminal 4. Configure Hadoop Go to \u0026ldquo;/usr/local/Cellar/hadoop-2.8.4/etc/hadoop\u0026rdquo;. You will see a bunch of files, among which you need to modify 6 files.\n  yarn-site.xml\n  Yarn is resource manager. The yarn.nodemanager.aux-services property tells NodeManagers that an auxiliary service, called mapreduce.shuffle, will be needed. Afterward, we give the class name to use for shuffling.\n  Replace a block\n  \u0026lt;configuration\u0026gt; ... \u0026lt;/configuration\u0026gt;     into\n  \u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.nodemanager.aux-services\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;mapreduce_shuffle\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.nodemanager.auxservices.mapreduce.shuffle.class\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;org.apache.hadoop.mapred.ShuffleHandler\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt;       core-site.xml\n  core-site is a website showing the overview of running Hadoop. We will specify the directory for temp files and set the port of the website as 9000.\n  Replace a block\n  \u0026lt;configuration\u0026gt; ... \u0026lt;/configuration\u0026gt;     into\n  \u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hadoop.tmp.dir\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;/usr/local/Cellar/hadoop/hdfs/tmp\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;fs.defaultFS\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;hdfs://localhost:9000\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt;       hdfs-site.xml\n  This is the configuration for HDFS. We specify the number of replications for each file and the directories of namenode, secondary namenode, and datanode. We set the permission checking as false.\n  Replace a block\n  \u0026lt;configuration\u0026gt; ... \u0026lt;/configuration\u0026gt;     into\n  \u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;dfs.replication\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;2\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;dfs.permissions\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;false\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;dfs.namenode.name.dir\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;/usr/local/Cellar/hadoop-2.8.4/data/dfs/namenode\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;dfs.namenode.checkpoint.dir\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;/usr/local/Cellar/hadoop-2.8.4/data/dfs/namesecondary\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;dfs.datanode.data.dir\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;/usr/local/Cellar/hadoop-2.8.4/data/dfs/datanode\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt;       mapred-site.xml\n  It defines MapReduce framework for Hadoop. We specify which framework we will use to run MapReduce, and it run as a Yarn application. In addition, we set the website port.\n  Replace a block\n  \u0026lt;configuration\u0026gt; ... \u0026lt;/configuration\u0026gt;     into\n  \u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;mapred.job.tracker\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;localhost:9010\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;mapreduce.framework.name\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;yarn\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt;       hadoop-env.sh\n In this file, we specify environment variables for the JDK used by Hadoop. We need to change \u0026ldquo;export JAVA_HOME=$JAVA_HOME\u0026rdquo; into \u0026ldquo;export JAVA_HOME=YOUR_JAVA_DIRECTORY\u0026rdquo; (use your note from step1).    hadoop-env.cmd\n In addition to previous configuration, we need to change \u0026ldquo;set JAVA_HOME=$JAVA_HOME\u0026rdquo; into \u0026ldquo;set JAVA_HOME=YOUR_JAVA_DIRECTORY\u0026rdquo; (use your note from step1).    (Only for MacOS) You need to turn on Remote Login under System Preferences then File Sharing.\n(Only for Linux) You need to install SSH by yourself\n$ sudo apt-get install ssh rsync Then, we generate SSH as follows:\n$ ssh-keygen -t rsa $ cat ~/.ssh/id_rsa.pub \u0026gt;\u0026gt; ~/.ssh/authorized_keys 6. Start and run Hadoop To set up Hadoop, we need to format the namenode.\nAt your CMD, type\n$ hdfs namenode -format (It is supposed to be done only once. However, if your system setting changes (e.g., changing IP address), you need to format it again. In this case, you first need to clear /usr/local/Cellar/hadoop-2.8.4/data/)\nTo launch Hadoop, type\n$ start-all.sh $ jps //to check if it works Open Hadoop Management web page http://localhost:50070.\nTo stop,\n$ stop-all.sh You are DONE!\n","date":1579806207,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579806207,"objectID":"ef5414e4289626442a3b05bad31c751e","permalink":"https://gyunamister.github.io/post/hadoop-installation-mac-linux/","publishdate":"2020-01-23T20:03:27+01:00","relpermalink":"/post/hadoop-installation-mac-linux/","section":"post","summary":"The (easiest) Hadoop installation - MacOS/Linux (Last updated: 24. Jan. 2020 14:00)\nThis blog post is a supplement for Hadoop instruction at Introduction to Data Science, RWTH-Aachen. The goal is to make sure that everyone can run Hadoop at his/her PC. Thus, I aim for explaining how to install Hadoop in the easiest manner by taking the simplest approach. This is not a succint way to install Hadoop (If you think you are quite familar with how (system-related) things work, I recommend you to find other blog posts).","tags":["BigData","Hadoop","Installation"],"title":"The (easiest) Hadoop installation - MacOS/Linux","type":"post"},{"authors":["Gyunam Park"],"categories":[],"content":"The (easiest) Hadoop installation - Windows (Last updated: 24. Jan. 2020 14:00)\nThis blog post is a supplement for Hadoop instruction at Introduction to Data Science, RWTH-Aachen. The goal is to make sure that everyone can run Hadoop at his/her PC. Thus, I aim for explaining how to install Hadoop in the easiest manner by taking the simplest approach. This is not a succint way to install Hadoop (If you think you are quite familar with how (system-related) things work, I recommend you to find other blog posts).\nLet's get started.\n1. Install Java Hadoop is based on Java language, so you need to have Java SE Development Kit in your PC.\nYou can download it in the link below:\nhttps://www.oracle.com/technetwork/java/javase/downloads/jdk8downloads-2133151.html\n2. Make soft link If you do not specify the directory, JDK is installed at \u0026ldquo;Program Files\u0026rdquo;. The problem is that the space in the path name (e.g., C:\\Program Files\\Java\\jdk1.8.0_213 ) is not allowed in Hadoop configuration. To prevent this issue, we use softlink. The idea is simple. We do not physically move the JDK (you may already have several dependencies on it from different programs.). Instead, we make an artifitial link which connects to JDK.\nOpen terminal and type:\n$ mkdir C:\\tools $ mklink /j C:\\tools\\java \u0026quot;YOUR_JDK_DIRECTORY\u0026quot; 3. Download hadoop Download hadoop and unzip it at C:\\hadoop-2.8.4\n4. Configure System Environment We need to set Hadoop PATH in system environment. This step is required to use hadoop commands at CMD.\nAdd below to your system path\n C:\\hadoop-2.8.4\\bin C:\\hadoop-2.8.4\\sbin  5. Configure Hadoop Go to \u0026ldquo;C:\\hadoop-2.8.4\\etc\\hadoop\u0026rdquo;. You will see a bunch of files, among which you need to modify 6 files.\n  yarn-site.xml\n  Yarn is resource manager. The yarn.nodemanager.aux-services property tells NodeManagers that an auxiliary service, called mapreduce.shuffle, will be needed. Afterward, we give the class name to use for shuffling.\n  Replace a block\n  \u0026lt;configuration\u0026gt; ... \u0026lt;/configuration\u0026gt;     into\n  \u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.nodemanager.aux-services\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;mapreduce_shuffle\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.nodemanager.auxservices.mapreduce.shuffle.class\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;org.apache.hadoop.mapred.ShuffleHandler\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt;       core-site.xml\n  core-site is a website showing the overview of running Hadoop. We will specify the directory for temp files and set the port of the website as 9000.\n  Replace a block\n  \u0026lt;configuration\u0026gt; ... \u0026lt;/configuration\u0026gt;     into\n  \u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hadoop.tmp.dir\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;\\C:\\hadoop-2.8.4\\data\\tmp\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;fs.defaultFS\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;hdfs://localhost:9000\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt;       hdfs-site.xml\n  This is the configuration for HDFS. We specify the number of replications for each file and the directories of namenode, secondary namenode, and datanode. We set the permission checking as false.\n  Replace a block\n  \u0026lt;configuration\u0026gt; ... \u0026lt;/configuration\u0026gt;     into\n  \u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;dfs.replication\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;2\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;dfs.permissions\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;false\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;dfs.namenode.name.dir\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;\\C:/hadoop-2.8.4\\data\\dfs\\namenode\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;dfs.namenode.checkpoint.dir\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;\\C:/hadoop-2.8.4\\data\\dfs\\namesecondary\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;dfs.datanode.data.dir\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;\\C:/hadoop-2.8.4\\data\\dfs\\datanode\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt;       mapred-site.xml\n  It defines MapReduce framework for Hadoop. We specify which framework we will use to run MapReduce, and it run as a Yarn application. In addition, we set the website port.\n  Replace a block\n  \u0026lt;configuration\u0026gt; ... \u0026lt;/configuration\u0026gt;     into\n  \u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;mapred.job.tracker\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;localhost:9010\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;mapreduce.framework.name\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;yarn\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt;       hadoop-env.sh\n In this file, we specify environment variables for the JDK used by Hadoop. We need to change \u0026ldquo;export JAVA_HOME=$JAVA_HOME\u0026rdquo; into \u0026ldquo;export JAVA_HOME=C:\\tools\\java\u0026rdquo;.    hadoop-env.cmd\n In addition to previous configuration, we need to change \u0026ldquo;set JAVA_HOME=$JAVA_HOME\u0026rdquo; into \u0026ldquo;set JAVA_HOME=C:\\tools\\java\u0026rdquo;.    6. Start and run Hadoop To set up Hadoop, we need to format the namenode.\nAt your CMD, type\n$ hdfs namenode -format (It is supposed to be done only once. However, if your system setting changes (e.g., changing IP address), you need to format it again. In this case, you first need to clear C:\\hadoop-2.8.4\\data)\nTo launch Hadoop, type\n$ start-all $ jps //to check if it works Open Hadoop Management web page http://localhost:50070.\nTo stop,\n$ stop-all You are DONE!\n","date":1579805859,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579805859,"objectID":"347edc0ac3a52bad61269e49d7c32e73","permalink":"https://gyunamister.github.io/post/hadoop-installation/","publishdate":"2020-01-23T19:57:39+01:00","relpermalink":"/post/hadoop-installation/","section":"post","summary":"The (easiest) Hadoop installation - Windows (Last updated: 24. Jan. 2020 14:00)\nThis blog post is a supplement for Hadoop instruction at Introduction to Data Science, RWTH-Aachen. The goal is to make sure that everyone can run Hadoop at his/her PC. Thus, I aim for explaining how to install Hadoop in the easiest manner by taking the simplest approach. This is not a succint way to install Hadoop (If you think you are quite familar with how (system-related) things work, I recommend you to find other blog posts).","tags":["BigData","Hadoop","Installation"],"title":"The (easiest) Hadoop installation - Windows","type":"post"},{"authors":["Gyunam Park","Minseok Song"],"categories":[],"content":"","date":1577923200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577923200,"objectID":"f509e00951969654aef25f77b0279211","permalink":"https://gyunamister.github.io/publication/dss-2020/","publishdate":"2020-01-23T20:06:53+01:00","relpermalink":"/publication/dss-2020/","section":"publication","summary":"Online operational support is gaining increasing interest due to the availability of real-time data and sufficient computing power, such as predictive business process monitoring. Predictive business process monitoring aims at providing timely information that enables proactive and corrective actions to improve process enactments and mitigate risks. There are a handful of research works focusing on the predictions at the instance level. However, it is more practical to predict the performance of processes at the process model level and detect potential weaknesses in the process to facilitate the proactive actions that will improve the process execution. Thus, in this paper, we propose a novel method to predict the future performances of a business process at the process model level. More in detail, we construct an annotated transition system and generate a process representation matrix from it. Based on the process representation matrix, we build performance prediction models using deep neural networks that consider both spatial and temporal dependencies present in the underlying business process. To validate the proposed method, we performed case studies on three real-life logs.","tags":["Paper","Prediction","DSS"],"title":"Predicting performances in business processes using deep neural networks","type":"publication"},{"authors":["Gyunam Park","Minseok Song"],"categories":[],"content":"","date":1570752000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570752000,"objectID":"0bf5915041e07503835dd2db88656441","permalink":"https://gyunamister.github.io/publication/icpm-extended-2019/","publishdate":"2020-01-23T20:13:24+01:00","relpermalink":"/publication/icpm-extended-2019/","section":"publication","summary":"Predictive business process monitoring aims at providing predictions about running instances by analyzing logs of completed cases in a business process. Recently, a lot of research focuses on increasing productivity and efficiency in a business process by forecasting potential problems during its executions. However, most of the studies lack suggesting concrete actions to improve the process. They leave it up to the subjective judgment of a user. In this paper, we propose a novel method to connect the results from predictive business process monitoring to actual business process improvements. More in detail, we optimize the resource allocation in a non-clairvoyant online environment, where we have limited information required for scheduling, by exploiting the predictions. The proposed method integrates the offline prediction model construction that predicts the processing time and the next activity of an ongoing instance using Bayesian Neural Networks (BNNs) with the online resource allocation that is extended from the minimum cost and maximum flow algorithm. To validate the proposed method, we performed experiments using an artificial event log and a real-life event log from a global financial organization.","tags":[],"title":"Prediction-based Resource Allocation using Bayesian Neural Networks and Minimum Cost and Maximum Flow Algorithm","type":"publication"},{"authors":["Gyunam Park","Minseok Song"],"categories":[],"content":"","date":1561507200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561507200,"objectID":"880dcf44858c0e4d8c012cf6aec36b41","permalink":"https://gyunamister.github.io/publication/icpm-2019/","publishdate":"2020-01-23T20:12:37+01:00","relpermalink":"/publication/icpm-2019/","section":"publication","summary":"Predictive business process monitoring aims at providing the predictions about running instances by analyzing logs of completed cases of a business process. Recently, a lot of research focuses on increasing productivity and efficiency in a business process by forecasting potential problems during its executions. However, most of the studies lack suggesting concrete actions to improve the process. They leave it up to the subjective judgment of a user. In this paper, we propose a novel method to connect the results from predictive business process monitoring to actual business process improvements. More in detail, we optimize the resource allocation in a non-clairvoyant online environment, where we have limited information required for scheduling, by exploiting the predictions. The proposed method integrates offline prediction model construction that predicts the processing time and the next activity of an ongoing instance using LSTM with online resource allocation that is extended from the minimum cost and maximum flow algorithm. To validate the proposed method, we performed experiments using an artificial event log and a real-life event log from a global financial organization.","tags":["Paper","Recommendation","ICPM2019"],"title":"Prediction-based Resource Allocation using LSTM and Minimum Cost and Maximum Flow Algorithm","type":"publication"}]