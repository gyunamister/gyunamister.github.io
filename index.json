[{"authors":["admin"],"categories":null,"content":"Gyunam Park is currently a Ph.D. student at the chair of Process and Data Science (PADS), Computer Science, RWTH-Aahcen University since 2019. His research interest includes process mining, data science, online operational support, machine learning, and deep learning. He received M.Sc. degree in in the Department of Industrial \u0026amp; Management Engineering at POSTECH (Pohang University of Science and Technology), Pohang, South Korea. He received the B.S. degree in business administration and computer science from UNIST (Ulsan National Institute of Science and Technology), Ulsan, South Korea.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://gyunamister.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Gyunam Park is currently a Ph.D. student at the chair of Process and Data Science (PADS), Computer Science, RWTH-Aahcen University since 2019. His research interest includes process mining, data science, online operational support, machine learning, and deep learning. He received M.Sc. degree in in the Department of Industrial \u0026amp; Management Engineering at POSTECH (Pohang University of Science and Technology), Pohang, South Korea. He received the B.S. degree in business administration and computer science from UNIST (Ulsan National Institute of Science and Technology), Ulsan, South Korea.","tags":null,"title":"Gyunam Park","type":"authors"},{"authors":[],"categories":[],"content":"","date":1583020800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583020800,"objectID":"1c84e1d124e5cd010cafa725d72d98c3","permalink":"https://gyunamister.github.io/publication/wi-2020/","publishdate":"2020-01-23T20:12:43+01:00","relpermalink":"/publication/wi-2020/","section":"publication","summary":"Predictive business process monitoring (PBPM) deals with predicting a process's future behavior based on historical event logs to support a process's execution. Many of the recent techniques utilize a machine-learned model to predict which event type is the next most likely. Beyond PBPM, prescriptive BPM aims at finding optimal actions based on considering relevant key performance indicators. Existing techniques are geared towards the outcome prediction and deal with alarms for interventions or interventions that do not represent process events. In this paper, we argue that the next event prediction is insufficient for practitioners. Accordingly, this research-in-progress paper proposes a technique for determining next best actions that represent process events. We conducted an intermediate evaluation to test the usefulness and the quality of our technique compared to the most frequently cited technique for predicting next events. The results show a higher usefulness for process participants than a next most likely event.","tags":["Paper","Recommendation","WI2020"],"title":"From predictive to prescriptive process monitoring: Recommending the next best actions instead of calculating the next most likely events","type":"publication"},{"authors":["Gyunam Park"],"categories":[],"content":"Hadoop Hands-On (Process Mining with Hadoop) (Last updated: 31. January. 2020)\nThis blog post is a supplement for Hadoop instruction at Introduction to Data Science, RWTH-Aachen. This post covers:\n What is Hadoop Distributed File System (HDFS)? How can we use it? What is Hadoop MapReduce? How can we use it? How can we apply process mining techniques to an event log with billions of events with Hadoop?  We are living in the world of big data. Data is being generated at all the places we can imagine. If you look outside the window, people waiting for the bus generate huge amount of logs while surfing websites and watching Youtube clips. The houses also produce lots of data from sensors attached to electrical machines, even including bulbs. This enables companies to develop services (e.g., personalized recommendation) which benifits their customers (like us) a lot.\nIf you think of how the companies give those advantages to us, it is not coming for free. Companies are struggling to manage the data as efficient as possible. Building big data infrastructure is one of those efforts. How do they exploit big data infrastructure to manage the big data efficiently? There are three most important trends in constructing big data infrastructure\n Distribution More data in memory Streaming  In this post, I am going to deal with distribution part.\nIf we have a super computer which is capable of handling big data with fast computation and reliable fault tolerance, we don't really have to worry much about distributing the storage and computation into different machines. However, most of the time, it is not the case. Then, what can we do? The answer is distribution.\nThe motivation is that we store the massive volume of data into multiple cheap commodities and parallelize computation across CPUs of the commodities. With this simple idea, we are able to store and analyze big data. Then, how can we achieve it? The answer is Hadoop.\n(Wikipedia) Hadoop is a collection of open-source software utilities that facilitate using a network of many computers to solve problems involving massive amounts of data and computation. It provides a software framework for distributed storage and processing of big data using the MapReduce programming model.\nThe first phase of hadoop was composed of Hadoop Distributed File System (HDFS), which is used for storing data, and MapReduce programming model, which is used for distributing computation. The second phase of hadoop was more elaborated by separating the resource management functionality of previous MapReduce into Yarn and introducing more specialized applications like Hive, which is used for making queries. In the third phase, it becomes much more elaborated, and there are hundreds of applications available in the context of Hadoop framework, which are used for machine learning, streaming data analysis, cloud environment, etc.\n  Hadoop History   The best way to understand how Hadoop works is to learn about HDFS and MapReduce, which are basic building blocks for various other applications.\n1. HDFS If you upload a file into DFS, it is split into data blocks, and each of them is stored into different nodes. For the purpose of fault tolerance, you can make copies of those blocks and store them into different nodes. Let's say you have two files, file1.txt and file2.txt. They are divided into three and two blocks, respectively. Each block is copied three times, and stored into data nodes. For example, file1.txt is splited into three blocks, and the three copies of block A are stored at data node #1, #2, and #4. It gurantees fault tolerance, i.e., even though data node #1 fails, there are data node #2 and #4, which are still running.\n  HDFS example   So, how can we upload data into HDFS? Let's have a look at some basic commands for HDFS.\n1.1. Preparation For windows, open your CMD (or Anaconda prompt) with administrator role, and type below:\n$ hadoop dfsadmin -safemode leave $ start-all For Mac/Linux,\n$ start-all.sh 1.2. Commands   Cat: Displaying the contents of the filename on console or stdout\n$ hadoop fs -cat /file1   CopyFromLocal/CopyToLocal: uploading/downloding file from/to local\n$ hadoop fs –copyFromLocal file:///file1 /folder1 $ hadoop fs –copyFromLocal file:///folder1 /folder2 $ hadoop fs –copyFromLocal file:///file1 /folder1/file2   cp: relocating files in HDFS\n$ hadoop fs –cp /file1 /folder1 $ hadoop fs –cp /file1 /file2 /folder1 $ hadoop fs –cp /folder1 /folder2   ls: listing the files in the current directory\n$ hadoop fs –ls /folder1 $ hadoop fs –ls /file1   mkdir: making directory\n$ hadoop fs –mkdir /folder1 $ hadoop fs –mkdir –p /folder1/folder2/folder3   rm: removing file\n$ hadoop fs –rm -r /folder1 $ hadoop fs –rm /file1 $ hadoop fs –rm –r /folder1   1.3. Excercises  Build the folders /test/input in your HDFS Build the folders /test/output in your HDFS Copy the local file PriceSum1.txt into folder /test/input Show the contents of PriceSum1.txt in your terminal Delete the file /test/input/PriceSum1.txt  2. MapReduce We uploaded our files into HDFS, and they are splited into some blocks, copied, and stored into data nodes. So, what's next? It is time to do some actual computations using MapReduce Programming model. Let's do word counting with Hadoop.\n2.1. Concept Suppose we have a file, WordCount1.txt, which contains the following sentences:\n the quick brown fox the fox ate the mouse how now brown cow  Assume that this file is split into three blocks, each of which contains one sentence. How can we count the frequency of words in this file? Now, it's time for MapReduce (MR).\nMR consists of three functions, map, suffle, and reduce. Map function $map \\in K_1 \\times V_1 \\to (K_2 \\times V_2)^*$ maps tuples into sets of tuples.\n  Map example   For example, the block 1 (i.e., sentence 1), $(block_1, the ; quick ; brown ; fox)$ is mapped into ${ (the,1),(brown,1),(fox,1),(quick,1) }$. The block 2 (i.e., sentence 2), $(block_2,the ; fox ; the ; ate ; mouse)$, is mapped into ${ (the,1),(fox,1),(the,1),(ate,1),(mouse,1) }$.\nSuffle function $suffle \\in (K_2 \\times V_2)^* \\to K_2 \\times (V_2)^*$ maps sets of tuples into tuples of a key and a set.\n  Shuffle example   For example, $(brown,1)$ from block 1 and $(brown,1) $ from block 2 are mapped into $(brown,[1,1])$.\nReduce function $reduce \\in K_2 \\times (V_2)^* \\to (K_3 \\times V_3)^*$ maps tuples of a key and a set to sets of tuples.\n  Reduce example   2.2. Excercise   For the input document, calculate the total price for each invoice ID. Presume you use MapReduce to do this, please write down the output of each Map function, the output after shuffle, and the output of Reduce function.\n  Input for Price Sum     3. MapReduce Programming with Python So far, we have learned what MapReduce is and how it works. Now, let our Hadoop framework do what we did by hand. Let's first recap what's done by hand. Given an input, we applied map function, shuffle function, and reduce function. Then, what do we need to do for Hadoop to do it instead of us?\n  First, upload file into HDFS (Give input)\n  Write Map function (in Python)\n(Shuffling is done by Hadoop)\n  Write Reduce function (in Python)\n  Write Command\n  3.1. Uploading file into HDFS. Also, see 1.2.\n$ hadoop fs -copyFromLocal ./WordCount1.txt /test/input 3.2. Write Map function #!/usr/bin/env python import sys # input comes from STDIN for sentence in sys.stdin: # remove whitespace sentence = sentence.strip() # split the sentence into words words = sentence.split() # increase counters for word in words: # write the results to STDOUT; # key: word, value: 1 (count of the word) print(\u0026#39;%s\\t%s\u0026#39; % (word, 1)) 3.3. Write Reduce function #!/usr/bin/env python from operator import itemgetter import sys current_word = None current_count = 0 word = None # input comes from STDIN for kv_pair in sys.stdin: # remove whitespace kv_pair = kv_pair.strip() # parse the input (word,count) we got from mapper.py word, count = kv_pair.split(\u0026#39;\\t\u0026#39;, 1) # convert count (currently a string) to int count = int(count) # shuflling is done by Hadoop if current_word!=word: if current_word: # write result to STDOUT print(\u0026#39;%s\\t%s\u0026#39; % (current_word, current_count)) current_word = word current_count = count else: current_count += count # output the last word if current_word == word: print(\u0026#39;%s\\t%s\u0026#39; % (current_word, current_count)) 3.4. Command For Windows:\n$ hadoop jar C:\\hadoop-2.8.4\\share\\hadoop\\tools\\lib\\hadoop-streaming-2.8.4.jar -input hdfs:///test/input/WordCount1.txt -output hdfs:///test/output/WordCountOutput0 -mapper \u0026quot;python C:\\Users\\park\\Desktop\\bigdata\\new_instruction\\word_mapper.py\u0026quot; -reducer \u0026quot;python C:\\Users\\park\\Desktop\\bigdata\\new_instruction\\word_reducer.py\u0026quot; -file C:\\Users\\park\\Desktop\\bigdata\\new_instruction\\word_mapper.py -file C:\\Users\\park\\Desktop\\bigdata\\new_instruction\\word_reducer.py For Mac\u0026amp;Linux:\nhadoop jar /usr/local/Cellar/hadoop-2.8.4/share/hadoop/tools/lib/hadoop-streaming-2.8.4.jar \\ -file /Users/GYUNAM/Desktop/bigdata/instruction/word_mapper.py \\ -mapper \u0026quot;python word_mapper.py\u0026quot; \\ -file /Users/GYUNAM/Desktop/bigdata/instruction/word_reducer.py \\ -reducer \u0026quot;python word_reducer.py\u0026quot; \\ -input /test/input/WordCount1.txt \\ -output /test/output/WordCountOutput 3.5. Excercise Write down the Python code of mapper and reducer to solve the problem from exercise 1: calculate the total price for each invoice id for a given document with the same format as shown in exercise 1. Run your code over the file PriceSum1.txt stored in HDFS\n4. Process Mining with Hadoop Let's say we have an event log with billions of events. How can we discover process model from this event log?\nThe answer is to use Hadoop framework. We can distribute the event log into multiple data nodes, and apply MapReduce programming model to compute directly follows relations (i.e., how many times activity x is followed by activity y). For this, we need to apply two MapReduce tasks. The first task is to generate traces from the event log by using MapReduce. The second is to discover directly follows relations (DFR) from the traces. Afterward, this direclty follows relations to discover a directly follows graph (DFG) and a workflow net.\nBelow is the overview of our approach\n  Overview   4.1. MapReduce Task (1) Below is the description of how it works:\n  Overview   4.1.1 Map function #!/usr/bin/env python import sys # input comes from STDIN (standard input) for line in sys.stdin: # remove whitespace and split row into values line_split = line.strip().split(\u0026#34;\\t\u0026#34;) # assign case, activity, timestamp case = line_split[0] activity = line_split[1] timestamp = line_split[3] # write the results to STDOUT; # key: case, value: (timestamp,activity) print(\u0026#39;%s\\t%s\\t%s\u0026#39; % (case, timestamp, activity)) 4.1.2 Reduce function #!/usr/bin/env python import sys import json current_case = None # input comes from STDIN for line in sys.stdin: # remove whitespace and parse the input (case,(timestamp, activity)) we got from mapper.py line_split = line.strip().split(\u0026#34;\\t\u0026#34;) caseid, timestamp, activity = line_split[0], line_split[1], line_split[2] # shuflling is done by Hadoop if caseid != current_case: # write result to STDOUT if current_case: print(\u0026#39;%s\\t%s\u0026#39; % (current_case,json.dumps(current_trace))) # reset current trace current_case = caseid current_trace = list() current_trace += [activity] else: current_trace += [activity] # output the last word if current_case == caseid: print(\u0026#39;%s\\t%s\u0026#39; % (caseid,json.dumps(current_trace))) 4.2. MapReduce Task (2) Below is the description of how it works:\n  Overview   4.2.1. Map function #!/usr/bin/env python import sys import json # input comes from STDIN for line in sys.stdin: # remove whitespace and split row into values line_split = line.strip().split(\u0026#34;\\t\u0026#34;) # load trace into list of activities activities = json.loads(line_split[1]) for i in range(len(activities)-1): # write the results to STDOUT; # key: (from,to), value: 1 (count of the relation) stru = activities[i] + \u0026#34;,\u0026#34; + activities[i + 1] print(\u0026#39;%s\\t%s\u0026#39; % (stru, 1)) 4.2.2. Reduce function #!/usr/bin/env python \u0026#34;\u0026#34;\u0026#34;reducer.py\u0026#34;\u0026#34;\u0026#34; from operator import itemgetter import sys current_relation = None current_count = 0 relation = None # input comes from STDIN for line in sys.stdin: # remove whitespace line = line.strip() # parse the input ((from,to),count) we got from mapper.py relation, count = line.split(\u0026#39;\\t\u0026#39;, 1) # convert count (currently a string) to int count = int(count) # shuflling is done by Hadoop if current_relation!=relation: if current_relation: # write result to STDOUT print(\u0026#39;%s\\t%s\u0026#39; % (current_relation, current_count)) current_relation = relation current_count = count else: current_count += count # output the last relation if current_relation == relation: print(\u0026#39;%s\\t%s\u0026#39; % (current_relation, current_count)) 4.3. Command 4.3.1. MapReduce Task (1) #For Windows users !hadoop jar C:\\hadoop-2.8.4\\share\\hadoop\\tools\\lib\\hadoop-streaming-2.8.4.jar -file C:\\hadoop-handson\\pm_mapper1.py -mapper \u0026quot;python C:\\hadoop-handson\\pm_mapper1.py\u0026quot; -file C:\\hadoop-handson\\pm_reducer1.py -reducer \u0026quot;python C:\\hadoop-handson\\pm_reducer1.py\u0026quot; -input hdfs:///test/input/running-example.tsv -output hdfs:///test/output/DFG0 #For Mac/Linux users !hadoop jar /usr/local/Cellar/hadoop-2.8.4/share/hadoop/tools/lib/hadoop-streaming-2.8.4.jar \\ -file /Users/GYUNAM/Desktop/bigdata/instruction/pm_mapper1.py \\ -mapper \u0026quot;python pm_mapper1.py\u0026quot; \\ -file /Users/GYUNAM/Desktop/bigdata/instruction/pm_reducer1.py \\ -reducer \u0026quot;python pm_reducer1.py\u0026quot; \\ -input /test/input/running-example.tsv \\ -output /test/output/DFG0 4.3.2. MapReduce Task (2) #For Windows users !hadoop jar C:\\hadoop-2.8.4\\share\\hadoop\\tools\\lib\\hadoop-streaming-2.8.4.jar -file C:\\hadoop-handson\\pm_mapper2.py -mapper \u0026quot;python C:\\hadoop-handson\\pm_mapper2.py\u0026quot; -file C:\\hadoop-handson\\pm_reducer2.py -reducer \u0026quot;python C:\\hadoop-handson\\pm_reducer2.py\u0026quot; -input hdfs:///test/output/DFG0/part-00000 -output hdfs:///test/output/DFG0-final #For Mac/Linux users !hadoop jar /usr/local/Cellar/hadoop-2.8.4/share/hadoop/tools/lib/hadoop-streaming-2.8.4.jar \\ -file /Users/GYUNAM/Desktop/bigdata/instruction/pm_mapper2.py \\ -mapper \u0026quot;python pm_mapper2.py\u0026quot; \\ -file /Users/GYUNAM/Desktop/bigdata/instruction/pm_reducer2.py \\ -reducer \u0026quot;python pm_reducer2.py\u0026quot; \\ -input /test/output/DFG0/part-00000 \\ -output /test/output/DFG0-final 4.3.3. Copy output from HDFS to Local #For Windows users !hadoop fs -copyToLocal /test/output/DFG0-final/part-00000 C:\\hadoop-handson\\dfr1.txt #For Mac/Linux users !hadoop fs -copyToLocal /test/output/DFG0-final/part-00000 ./dfr1.txt 4.4 Process Discovery For applying process mining techniques, we use PM4PY.\n# 1. Import libraries import os import csv from pm4py.objects.log.importer.xes import factory as xes_importer from pm4py.objects.conversion.dfg import factory as dfg_mining_factory from pm4py.algo.discovery.dfg import factory as dfg_factory from pm4py.visualization.dfg import factory as dfg_vis_factory from pm4py.visualization.petrinet import factory as pn_vis_factory # 2. preprocessing with open(\u0026#39;dfr1.txt\u0026#39;) as file: file_reader = csv.reader(file, delimiter=\u0026#39;\\t\u0026#39;) dfg = dict() for row in file_reader: _from,_to=row[0].split(\u0026#39;,\u0026#39;) rel = (_from,_to) freq = int(row[1]) dfg[rel] = freq # 3. Visualize Directly-follows-graph (DFG) gviz = dfg_vis_factory.apply(dfg) dfg_vis_factory.view(gviz) # 4. Discover and Visualize Workflow-Net net, im, fm = dfg_mining_factory.apply(dfg) gviz = pn_vis_factory.apply(net, im, fm) pn_vis_factory.view(gviz) ","date":1580453714,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580453714,"objectID":"64a754246efa0957177ec6beb0fe81a1","permalink":"https://gyunamister.github.io/post/hadoop-hands-on/","publishdate":"2020-01-31T07:55:14+01:00","relpermalink":"/post/hadoop-hands-on/","section":"post","summary":"Hadoop Hands-On (Process Mining with Hadoop) (Last updated: 31. January. 2020)\nThis blog post is a supplement for Hadoop instruction at Introduction to Data Science, RWTH-Aachen. This post covers:\n What is Hadoop Distributed File System (HDFS)? How can we use it? What is Hadoop MapReduce? How can we use it? How can we apply process mining techniques to an event log with billions of events with Hadoop?  We are living in the world of big data.","tags":[],"title":"Hadoop Hands-On (Process Mining with Hadoop)","type":"post"},{"authors":["Gyunam Park"],"categories":[],"content":"The (easiest) Hadoop installation - MacOS/Linux (Last updated: 24. Jan. 2020 14:00)\nThis blog post is a supplement for Hadoop instruction at Introduction to Data Science, RWTH-Aachen. The goal is to make sure that everyone can run Hadoop at his/her PC. Thus, I aim for explaining how to install Hadoop in the easiest manner by taking the simplest approach. This is not a succint way to install Hadoop (If you think you are quite familar with how (system-related) things work, I recommend you to find other blog posts).\nLet's get started.\n1. Install Java Hadoop is based on Java language, so you need to have Java SE Development Kit in your PC.\nYou can download it in the link below:\nhttps://www.oracle.com/technetwork/java/javase/downloads/jdk8downloads-2133151.html\n Take note of the installation directory. E.g., /Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/bin  2. Download hadoop Download hadoop and unzip it at /usr/local/Cellar/hadoop-2.8.4\n3. Configure System Environment We need to set Hadoop PATH in system environment. This step is required to use hadoop commands at Terminal (zsh).\nOpen Terminal (zsh)\n$ vi /etc/profile (/etc/zprofile) Insert below sentences to the file. (You can change to edit mode by typing i)\nexport HADOOP_HOME=/usr/local/Cellar/hadoop-2.8.4 export PATH=$PATH:$HADOOP_HOME/bin export PATH=$PATH:$HADOOP_HOME/sbin At your Terminal (zsh)\nsource /etc/profile (/etc/zprofile) echo $HADOOP_HOME //YOU MUST SEE /usr/local/Cellar/hadoop-2.8.4 on your terminal 4. Configure Hadoop Go to \u0026ldquo;/usr/local/Cellar/hadoop-2.8.4/etc/hadoop\u0026rdquo;. You will see a bunch of files, among which you need to modify 6 files.\n  yarn-site.xml\n  Yarn is resource manager. The yarn.nodemanager.aux-services property tells NodeManagers that an auxiliary service, called mapreduce.shuffle, will be needed. Afterward, we give the class name to use for shuffling.\n  Replace a block\n  \u0026lt;configuration\u0026gt; ... \u0026lt;/configuration\u0026gt;     into\n  \u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.nodemanager.aux-services\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;mapreduce_shuffle\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.nodemanager.auxservices.mapreduce.shuffle.class\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;org.apache.hadoop.mapred.ShuffleHandler\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt;       core-site.xml\n  core-site is a website showing the overview of running Hadoop. We will specify the directory for temp files and set the port of the website as 9000.\n  Replace a block\n  \u0026lt;configuration\u0026gt; ... \u0026lt;/configuration\u0026gt;     into\n  \u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hadoop.tmp.dir\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;/usr/local/Cellar/hadoop/hdfs/tmp\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;fs.defaultFS\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;hdfs://localhost:9000\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt;       hdfs-site.xml\n  This is the configuration for HDFS. We specify the number of replications for each file and the directories of namenode, secondary namenode, and datanode. We set the permission checking as false.\n  Replace a block\n  \u0026lt;configuration\u0026gt; ... \u0026lt;/configuration\u0026gt;     into\n  \u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;dfs.replication\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;2\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;dfs.permissions\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;false\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;dfs.namenode.name.dir\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;/usr/local/Cellar/hadoop-2.8.4/data/dfs/namenode\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;dfs.namenode.checkpoint.dir\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;/usr/local/Cellar/hadoop-2.8.4/data/dfs/namesecondary\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;dfs.datanode.data.dir\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;/usr/local/Cellar/hadoop-2.8.4/data/dfs/datanode\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt;       mapred-site.xml\n  It defines MapReduce framework for Hadoop. We specify which framework we will use to run MapReduce, and it run as a Yarn application. In addition, we set the website port.\n  Replace a block\n  \u0026lt;configuration\u0026gt; ... \u0026lt;/configuration\u0026gt;     into\n  \u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;mapred.job.tracker\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;localhost:9010\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;mapreduce.framework.name\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;yarn\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt;       hadoop-env.sh\n In this file, we specify environment variables for the JDK used by Hadoop. We need to change \u0026ldquo;export JAVA_HOME=$JAVA_HOME\u0026rdquo; into \u0026ldquo;export JAVA_HOME=YOUR_JAVA_DIRECTORY\u0026rdquo; (use your note from step1).    hadoop-env.cmd\n In addition to previous configuration, we need to change \u0026ldquo;set JAVA_HOME=$JAVA_HOME\u0026rdquo; into \u0026ldquo;set JAVA_HOME=YOUR_JAVA_DIRECTORY\u0026rdquo; (use your note from step1).    (Only for MacOS) You need to turn on Remote Login under System Preferences then File Sharing.\n(Only for Linux) You need to install SSH by yourself\n$ sudo apt-get install ssh rsync Then, we generate SSH as follows:\n$ ssh-keygen -t rsa $ cat ~/.ssh/id_rsa.pub \u0026gt;\u0026gt; ~/.ssh/authorized_keys 6. Start and run Hadoop To set up Hadoop, we need to format the namenode.\nAt your CMD, type\n$ hdfs namenode -format (It is supposed to be done only once. However, if your system setting changes (e.g., changing IP address), you need to format it again. In this case, you first need to clear /usr/local/Cellar/hadoop-2.8.4/data/)\nTo launch Hadoop, type\n$ start-all.sh $ jps //to check if it works Open Hadoop Management web page http://localhost:50070.\nTo stop,\n$ stop-all.sh You are DONE!\n","date":1579806207,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579806207,"objectID":"ef5414e4289626442a3b05bad31c751e","permalink":"https://gyunamister.github.io/post/hadoop-installation-mac-linux/","publishdate":"2020-01-23T20:03:27+01:00","relpermalink":"/post/hadoop-installation-mac-linux/","section":"post","summary":"The (easiest) Hadoop installation - MacOS/Linux (Last updated: 24. Jan. 2020 14:00)\nThis blog post is a supplement for Hadoop instruction at Introduction to Data Science, RWTH-Aachen. The goal is to make sure that everyone can run Hadoop at his/her PC. Thus, I aim for explaining how to install Hadoop in the easiest manner by taking the simplest approach. This is not a succint way to install Hadoop (If you think you are quite familar with how (system-related) things work, I recommend you to find other blog posts).","tags":["BigData","Hadoop","Installation"],"title":"The (easiest) Hadoop installation - MacOS/Linux","type":"post"},{"authors":["Gyunam Park"],"categories":[],"content":"The (easiest) Hadoop installation - Windows (Last updated: 24. Jan. 2020 14:00)\nThis blog post is a supplement for Hadoop instruction at Introduction to Data Science, RWTH-Aachen. The goal is to make sure that everyone can run Hadoop at his/her PC. Thus, I aim for explaining how to install Hadoop in the easiest manner by taking the simplest approach. This is not a succint way to install Hadoop (If you think you are quite familar with how (system-related) things work, I recommend you to find other blog posts).\nLet's get started.\n1. Install Java Hadoop is based on Java language, so you need to have Java SE Development Kit in your PC.\nYou can download it in the link below:\nhttps://www.oracle.com/technetwork/java/javase/downloads/jdk8downloads-2133151.html\n2. Make soft link If you do not specify the directory, JDK is installed at \u0026ldquo;Program Files\u0026rdquo;. The problem is that the space in the path name (e.g., C:\\Program Files\\Java\\jdk1.8.0_213 ) is not allowed in Hadoop configuration. To prevent this issue, we use softlink. The idea is simple. We do not physically move the JDK (you may already have several dependencies on it from different programs.). Instead, we make an artifitial link which connects to JDK.\nOpen terminal and type:\n$ mkdir C:\\tools $ mklink /j C:\\tools\\java \u0026quot;YOUR_JDK_DIRECTORY\u0026quot; 3. Download hadoop Download hadoop and unzip it at C:\\hadoop-2.8.4\n4. Configure System Environment We need to set Hadoop PATH in system environment. This step is required to use hadoop commands at CMD.\nAdd below to your system path\n C:\\hadoop-2.8.4\\bin C:\\hadoop-2.8.4\\sbin  5. Configure Hadoop Go to \u0026ldquo;C:\\hadoop-2.8.4\\etc\\hadoop\u0026rdquo;. You will see a bunch of files, among which you need to modify 6 files.\n  yarn-site.xml\n  Yarn is resource manager. The yarn.nodemanager.aux-services property tells NodeManagers that an auxiliary service, called mapreduce.shuffle, will be needed. Afterward, we give the class name to use for shuffling.\n  Replace a block\n  \u0026lt;configuration\u0026gt; ... \u0026lt;/configuration\u0026gt;     into\n  \u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.nodemanager.aux-services\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;mapreduce_shuffle\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.nodemanager.auxservices.mapreduce.shuffle.class\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;org.apache.hadoop.mapred.ShuffleHandler\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt;       core-site.xml\n  core-site is a website showing the overview of running Hadoop. We will specify the directory for temp files and set the port of the website as 9000.\n  Replace a block\n  \u0026lt;configuration\u0026gt; ... \u0026lt;/configuration\u0026gt;     into\n  \u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hadoop.tmp.dir\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;\\C:\\hadoop-2.8.4\\data\\tmp\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;fs.defaultFS\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;hdfs://localhost:9000\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt;       hdfs-site.xml\n  This is the configuration for HDFS. We specify the number of replications for each file and the directories of namenode, secondary namenode, and datanode. We set the permission checking as false.\n  Replace a block\n  \u0026lt;configuration\u0026gt; ... \u0026lt;/configuration\u0026gt;     into\n  \u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;dfs.replication\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;2\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;dfs.permissions\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;false\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;dfs.namenode.name.dir\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;\\C:/hadoop-2.8.4\\data\\dfs\\namenode\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;dfs.namenode.checkpoint.dir\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;\\C:/hadoop-2.8.4\\data\\dfs\\namesecondary\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;dfs.datanode.data.dir\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;\\C:/hadoop-2.8.4\\data\\dfs\\datanode\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt;       mapred-site.xml\n  It defines MapReduce framework for Hadoop. We specify which framework we will use to run MapReduce, and it run as a Yarn application. In addition, we set the website port.\n  Replace a block\n  \u0026lt;configuration\u0026gt; ... \u0026lt;/configuration\u0026gt;     into\n  \u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;mapred.job.tracker\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;localhost:9010\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;mapreduce.framework.name\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;yarn\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt;       hadoop-env.sh\n In this file, we specify environment variables for the JDK used by Hadoop. We need to change \u0026ldquo;export JAVA_HOME=$JAVA_HOME\u0026rdquo; into \u0026ldquo;export JAVA_HOME=C:\\tools\\java\u0026rdquo;.    hadoop-env.cmd\n In addition to previous configuration, we need to change \u0026ldquo;set JAVA_HOME=$JAVA_HOME\u0026rdquo; into \u0026ldquo;set JAVA_HOME=C:\\tools\\java\u0026rdquo;.    6. Start and run Hadoop To set up Hadoop, we need to format the namenode.\nAt your CMD, type\n$ hdfs namenode -format (It is supposed to be done only once. However, if your system setting changes (e.g., changing IP address), you need to format it again. In this case, you first need to clear C:\\hadoop-2.8.4\\data)\nTo launch Hadoop, type\n$ start-all $ jps //to check if it works Open Hadoop Management web page http://localhost:50070.\nTo stop,\n$ stop-all You are DONE!\n","date":1579805859,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579805859,"objectID":"347edc0ac3a52bad61269e49d7c32e73","permalink":"https://gyunamister.github.io/post/hadoop-installation/","publishdate":"2020-01-23T19:57:39+01:00","relpermalink":"/post/hadoop-installation/","section":"post","summary":"The (easiest) Hadoop installation - Windows (Last updated: 24. Jan. 2020 14:00)\nThis blog post is a supplement for Hadoop instruction at Introduction to Data Science, RWTH-Aachen. The goal is to make sure that everyone can run Hadoop at his/her PC. Thus, I aim for explaining how to install Hadoop in the easiest manner by taking the simplest approach. This is not a succint way to install Hadoop (If you think you are quite familar with how (system-related) things work, I recommend you to find other blog posts).","tags":["BigData","Hadoop","Installation"],"title":"The (easiest) Hadoop installation - Windows","type":"post"},{"authors":["Gyunam Park","Minseok Song"],"categories":[],"content":"","date":1577923200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577923200,"objectID":"f509e00951969654aef25f77b0279211","permalink":"https://gyunamister.github.io/publication/dss-2020/","publishdate":"2020-01-23T20:06:53+01:00","relpermalink":"/publication/dss-2020/","section":"publication","summary":"Online operational support is gaining increasing interest due to the availability of real-time data and sufficient computing power, such as predictive business process monitoring. Predictive business process monitoring aims at providing timely information that enables proactive and corrective actions to improve process enactments and mitigate risks. There are a handful of research works focusing on the predictions at the instance level. However, it is more practical to predict the performance of processes at the process model level and detect potential weaknesses in the process to facilitate the proactive actions that will improve the process execution. Thus, in this paper, we propose a novel method to predict the future performances of a business process at the process model level. More in detail, we construct an annotated transition system and generate a process representation matrix from it. Based on the process representation matrix, we build performance prediction models using deep neural networks that consider both spatial and temporal dependencies present in the underlying business process. To validate the proposed method, we performed case studies on three real-life logs.","tags":["Paper","Prediction","DSS"],"title":"Predicting performances in business processes using deep neural networks","type":"publication"},{"authors":["Gyunam Park","Minseok Song"],"categories":[],"content":"","date":1570752000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570752000,"objectID":"0bf5915041e07503835dd2db88656441","permalink":"https://gyunamister.github.io/publication/icpm-extended-2019/","publishdate":"2020-01-23T20:13:24+01:00","relpermalink":"/publication/icpm-extended-2019/","section":"publication","summary":"Predictive business process monitoring aims at providing predictions about running instances by analyzing logs of completed cases in a business process. Recently, a lot of research focuses on increasing productivity and efficiency in a business process by forecasting potential problems during its executions. However, most of the studies lack suggesting concrete actions to improve the process. They leave it up to the subjective judgment of a user. In this paper, we propose a novel method to connect the results from predictive business process monitoring to actual business process improvements. More in detail, we optimize the resource allocation in a non-clairvoyant online environment, where we have limited information required for scheduling, by exploiting the predictions. The proposed method integrates the offline prediction model construction that predicts the processing time and the next activity of an ongoing instance using Bayesian Neural Networks (BNNs) with the online resource allocation that is extended from the minimum cost and maximum flow algorithm. To validate the proposed method, we performed experiments using an artificial event log and a real-life event log from a global financial organization.","tags":[],"title":"Prediction-based Resource Allocation using Bayesian Neural Networks and Minimum Cost and Maximum Flow Algorithm","type":"publication"},{"authors":["Gyunam Park","Minseok Song"],"categories":[],"content":"","date":1561507200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561507200,"objectID":"880dcf44858c0e4d8c012cf6aec36b41","permalink":"https://gyunamister.github.io/publication/icpm-2019/","publishdate":"2020-01-23T20:12:37+01:00","relpermalink":"/publication/icpm-2019/","section":"publication","summary":"Predictive business process monitoring aims at providing the predictions about running instances by analyzing logs of completed cases of a business process. Recently, a lot of research focuses on increasing productivity and efficiency in a business process by forecasting potential problems during its executions. However, most of the studies lack suggesting concrete actions to improve the process. They leave it up to the subjective judgment of a user. In this paper, we propose a novel method to connect the results from predictive business process monitoring to actual business process improvements. More in detail, we optimize the resource allocation in a non-clairvoyant online environment, where we have limited information required for scheduling, by exploiting the predictions. The proposed method integrates offline prediction model construction that predicts the processing time and the next activity of an ongoing instance using LSTM with online resource allocation that is extended from the minimum cost and maximum flow algorithm. To validate the proposed method, we performed experiments using an artificial event log and a real-life event log from a global financial organization.","tags":["Paper","Recommendation","ICPM2019"],"title":"Prediction-based Resource Allocation using LSTM and Minimum Cost and Maximum Flow Algorithm","type":"publication"}]