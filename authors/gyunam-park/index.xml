<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Gyunam Park on Gyunam</title>
    <link>https://gyunamister.github.io/authors/gyunam-park/</link>
    <description>Recent content in Gyunam Park on Gyunam</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 23 Jan 2020 20:03:27 +0100</lastBuildDate>
    
	    <atom:link href="https://gyunamister.github.io/authors/gyunam-park/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>The (easiest) Hadoop installation - MacOS/Linux</title>
      <link>https://gyunamister.github.io/post/hadoop-installation-mac-linux/</link>
      <pubDate>Thu, 23 Jan 2020 20:03:27 +0100</pubDate>
      
      <guid>https://gyunamister.github.io/post/hadoop-installation-mac-linux/</guid>
      <description>&lt;h1 id=&#34;the-easiest-hadoop-installation---macoslinux&#34;&gt;The (easiest) Hadoop installation - MacOS/Linux&lt;/h1&gt;
&lt;p&gt;This blog post is a supplement for Hadoop instruction at &lt;em&gt;Introduction to Data Science, RWTH-Aachen&lt;/em&gt;. The goal is to make sure that everyone can run Hadoop at his/her PC. Thus, I aim for explaining how to install Hadoop in the easiest manner by taking the simplest approach. This is not a succint way to install Hadoop (If you think you are quite familar with how (system-related) things work, I recommend you to find other blog posts).&lt;/p&gt;
&lt;p&gt;Let&#39;s get started.&lt;/p&gt;
&lt;h3 id=&#34;1-install-java&#34;&gt;1. Install Java&lt;/h3&gt;
&lt;p&gt;Hadoop is based on Java language, so you need to have Java SE Development Kit in your PC.&lt;/p&gt;
&lt;p&gt;You can download it in the link below:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.oracle.com/technetwork/java/javase/downloads/jdk8downloads-2133151.html&#34;&gt;https://www.oracle.com/technetwork/java/javase/downloads/jdk8downloads-2133151.html&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Take note of the installation directory. E.g., /Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/bin&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;2-download-hadoop&#34;&gt;2. Download hadoop&lt;/h3&gt;
&lt;p&gt;Download &lt;a href=&#34;https://archive.apache.org/dist/hadoop/core/hadoop-2.8.4/&#34;&gt;hadoop&lt;/a&gt; and unzip it at /usr/local/Cellar/hadoop-2.8.4&lt;/p&gt;
&lt;h3 id=&#34;3-configure-system-environment&#34;&gt;3. Configure System Environment&lt;/h3&gt;
&lt;p&gt;We need to set Hadoop PATH in system environment. This step is required to use hadoop commands at Terminal (zsh).&lt;/p&gt;
&lt;p&gt;Open Terminal (zsh)&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ vi /etc/profile (/etc/zprofile)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Insert below sentences to the file. (You can change to edit mode by typing &lt;strong&gt;i&lt;/strong&gt;)&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;export HADOOP_HOME=/usr/local/Cellar/hadoop-2.8.4
export PATH=$PATH:$HADOOP_HOME/bin
export PATH=$PATH:$HADOOP_HOME/sbin
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;At your Terminal (zsh)&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;source /etc/profile (/etc/zprofile)
echo $HADOOP_HOME //YOU MUST SEE /usr/local/Cellar/hadoop-2.8.4 on your terminal
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;4-configure-hadoop&#34;&gt;4. Configure Hadoop&lt;/h3&gt;
&lt;p&gt;Go to &amp;ldquo;/usr/local/Cellar/hadoop-2.8.4/etc/hadoop&amp;rdquo;. You will see a bunch of files, among which you need to modify 6 files.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;yarn-site.xml&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Yarn is resource manager. The yarn.nodemanager.aux-services property tells NodeManagers that an auxiliary service, called mapreduce.shuffle, will be needed. Afterward, we give the class name to use for shuffling.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Replace a block&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;​	&amp;hellip;&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;into&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;core-site.xml&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;core-site is a website showing the overview of running Hadoop. We will specify the directory for temp files and set the port of the website as 9000.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Replace a block&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;​	&amp;hellip;&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;into&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;hdfs-site.xml&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;This is the configuration for HDFS. We specify the number of replications for each file and the directories of namenode, secondary namenode, and datanode. We set the permission checking as false.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Replace a block&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;​	&amp;hellip;&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;into&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;mapred-site.xml&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;It defines MapReduce framework for Hadoop. We specify which framework we will use to run MapReduce, and it run as a Yarn application. In addition, we set the website port.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Replace a block&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;​	&amp;hellip;&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;into&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;hadoop-env.sh&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In this file, we specify environment variables for the JDK used by Hadoop.&lt;/li&gt;
&lt;li&gt;We need to change &amp;ldquo;export JAVA_HOME=$JAVA_HOME&amp;rdquo; into &amp;ldquo;export JAVA_HOME=YOUR_JAVA_DIRECTORY&amp;rdquo; (use your note from step1).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;hadoop-env.cmd&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In addition to previous configuration, we need to change &amp;ldquo;set JAVA_HOME=$JAVA_HOME&amp;rdquo; into &amp;ldquo;set JAVA_HOME=YOUR_JAVA_DIRECTORY&amp;rdquo; (use your note from step1).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;(Only for MacOS) You need to turn on &lt;em&gt;Remote Login&lt;/em&gt; under &lt;strong&gt;System Preferences&lt;/strong&gt; then &lt;strong&gt;File Sharing&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;(Only for Linux) You need to install SSH by yourself&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ sudo apt-get install ssh rsync
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Then, we generate SSH as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ ssh-keygen -t rsa
$ cat ~/.ssh/id_rsa.pub &amp;gt;&amp;gt; ~/.ssh/authorized_keys
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;6-start-and-run-hadoop&#34;&gt;6. Start and run Hadoop&lt;/h3&gt;
&lt;p&gt;To set up Hadoop, we need to format the namenode.&lt;/p&gt;
&lt;p&gt;At your CMD, type&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ hdfs namenode -format
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;(It is supposed to be done only once. However, if your system setting changes (e.g., changing IP address), you need to format it again. In this case, you first need to clear /usr/local/Cellar/hadoop-2.8.4/data/)&lt;/p&gt;
&lt;p&gt;To launch Hadoop, type&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ start-all.sh
$ jps //to check if it works
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Open Hadoop Management web page http://localhost:50070.&lt;/p&gt;
&lt;p&gt;To stop,&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ stop-all.sh
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;You are DONE!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The (easiest) Hadoop installation - Windows</title>
      <link>https://gyunamister.github.io/post/hadoop-installation/</link>
      <pubDate>Thu, 23 Jan 2020 19:57:39 +0100</pubDate>
      
      <guid>https://gyunamister.github.io/post/hadoop-installation/</guid>
      <description>&lt;h1 id=&#34;the-easiest-hadoop-installation---windows&#34;&gt;The (easiest) Hadoop installation - Windows&lt;/h1&gt;
&lt;p&gt;This blog post is a supplement for Hadoop instruction at &lt;em&gt;Introduction to Data Science, RWTH-Aachen&lt;/em&gt;. The goal is to make sure that everyone can run Hadoop at his/her PC. Thus, I aim for explaining how to install Hadoop in the easiest manner by taking the simplest approach. This is not a succint way to install Hadoop (If you think you are quite familar with how (system-related) things work, I recommend you to find other blog posts).&lt;/p&gt;
&lt;p&gt;Let&#39;s get started.&lt;/p&gt;
&lt;h3 id=&#34;1-install-java&#34;&gt;1. Install Java&lt;/h3&gt;
&lt;p&gt;Hadoop is based on Java language, so you need to have Java SE Development Kit in your PC.&lt;/p&gt;
&lt;p&gt;You can download it in the link below:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.oracle.com/technetwork/java/javase/downloads/jdk8downloads-2133151.html&#34;&gt;https://www.oracle.com/technetwork/java/javase/downloads/jdk8downloads-2133151.html&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;2-make-soft-link&#34;&gt;2. Make soft link&lt;/h3&gt;
&lt;p&gt;If you do not specify the directory, JDK is installed at &amp;ldquo;Program Files&amp;rdquo;. The problem is that the space in the path name (e.g., C:\Program Files\Java\jdk1.8.0_213 ) is not allowed in Hadoop configuration. To prevent this issue, we use &lt;strong&gt;softlink&lt;/strong&gt;. The idea is simple. We do not physically move the JDK (you may already have several dependencies on it from different programs.). Instead, we make an artifitial link which connects to JDK.&lt;/p&gt;
&lt;p&gt;Open terminal and type:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ mkdir C:\tools
$ mklink /j C:\tools\java &amp;quot;YOUR_JDK_DIRECTORY&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;3-download-hadoop&#34;&gt;3. Download hadoop&lt;/h3&gt;
&lt;p&gt;Download &lt;a href=&#34;https://archive.apache.org/dist/hadoop/core/hadoop-2.8.4/&#34;&gt;hadoop&lt;/a&gt; and unzip it at C:\hadoop-2.8.4&lt;/p&gt;
&lt;h3 id=&#34;4-configure-system-environment&#34;&gt;4. Configure System Environment&lt;/h3&gt;
&lt;p&gt;We need to set Hadoop PATH in system environment. This step is required to use hadoop commands at CMD.&lt;/p&gt;
&lt;p&gt;Add below to your system path&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;C:\hadoop-2.8.4\bin&lt;/li&gt;
&lt;li&gt;C:\hadoop-2.8.4\sbin&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;5-configure-hadoop&#34;&gt;5. Configure Hadoop&lt;/h3&gt;
&lt;p&gt;Go to &amp;ldquo;C:\hadoop-2.8.4\etc\hadoop&amp;rdquo;. You will see a bunch of files, among which you need to modify 6 files.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;yarn-site.xml&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Yarn is resource manager. The yarn.nodemanager.aux-services property tells NodeManagers that an auxiliary service, called mapreduce.shuffle, will be needed. Afterward, we give the class name to use for shuffling.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Replace a block&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;​	&amp;hellip;&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;into&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;core-site.xml&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;core-site is a website showing the overview of running Hadoop. We will specify the directory for temp files and set the port of the website as 9000.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Replace a block&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;​	&amp;hellip;&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;into&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;hdfs-site.xml&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;This is the configuration for HDFS. We specify the number of replications for each file and the directories of namenode, secondary namenode, and datanode. We set the permission checking as false.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Replace a block&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;​	&amp;hellip;&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;into&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;mapred-site.xml&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;It defines MapReduce framework for Hadoop. We specify which framework we will use to run MapReduce, and it run as a Yarn application. In addition, we set the website port.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Replace a block&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;​	&amp;hellip;&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;into&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;hadoop-env.sh&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In this file, we specify environment variables for the JDK used by Hadoop.&lt;/li&gt;
&lt;li&gt;We need to change &amp;ldquo;export JAVA_HOME=$JAVA_HOME&amp;rdquo; into &amp;ldquo;export JAVA_HOME=C:\tools\java&amp;rdquo;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;hadoop-env.cmd&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In addition to previous configuration, we need to change &amp;ldquo;set JAVA_HOME=$JAVA_HOME&amp;rdquo; into &amp;ldquo;set JAVA_HOME=C:\tools\java&amp;rdquo;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;6-start-and-run-hadoop&#34;&gt;6. Start and run Hadoop&lt;/h3&gt;
&lt;p&gt;To set up Hadoop, we need to format the namenode.&lt;/p&gt;
&lt;p&gt;At your CMD, type&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ hdfs namenode -format
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;(It is supposed to be done only once. However, if your system setting changes (e.g., changing IP address), you need to format it again. In this case, you first need to clear C:\hadoop-2.8.4\data)&lt;/p&gt;
&lt;p&gt;To launch Hadoop, type&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ start-all
$ jps //to check if it works
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Open Hadoop Management web page http://localhost:50070.&lt;/p&gt;
&lt;p&gt;To stop,&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ stop-all
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;You are DONE!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Predicting performances in business processes using deep neural networks</title>
      <link>https://gyunamister.github.io/publication/dss-2020/</link>
      <pubDate>Thu, 02 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://gyunamister.github.io/publication/dss-2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Prediction-based Resource Allocation using Bayesian Neural Networks and Minimum Cost and Maximum Flow Algorithm</title>
      <link>https://gyunamister.github.io/publication/icpm-extended-2019/</link>
      <pubDate>Fri, 11 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>https://gyunamister.github.io/publication/icpm-extended-2019/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
